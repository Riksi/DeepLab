{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from string import ascii_lowercase\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_block(x):\n",
    "    if x.get_shape().as_list()[-1] != 3:\n",
    "        x = conv_bn_relu(x, ['conv0', 'bn_conv0'], kernel_size=1, filters=3)\n",
    "    x = conv_bn_relu(x, ['conv1', 'bn_conv1'], kernel_size=7, stride=2, filters=64)\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2)\n",
    "    return x\n",
    "\n",
    "def conv_bn(x, names, kernel_size, filters, stride=1, dilation_rate=1, training=False):\n",
    "    x = tf.layers.conv2d(x, kernel_size=kernel_size, strides=stride, filters=filters, \n",
    "                         dilation_rate=dilation_rate, padding='SAME', name=names[0])\n",
    "    x = tf.layers.batch_normalization(x, training=is_train, name=names[1])\n",
    "    return x\n",
    "    \n",
    "def conv_bn_relu(x, names, kernel_size, filters, stride=1, dilation_rate=1, training=False):\n",
    "    x = conv_bn(x, names, kernel_size, filters, stride, dilation_rate, training)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "    \n",
    "def conv_block(x, name, filters, stride=1, dilation_rate=1, training=False):\n",
    "    filters_in = x.get_shape().as_list()[-1]\n",
    "    filters_out = filters*4\n",
    "    out = conv_bn_relu(x, ['res%s_branch2a'%name, 'bn%s_branch2a'%name],\n",
    "                       kernel_size=1, stride=stride, filters=filters, training=training)\n",
    "    out = conv_bn_relu(out, ['res%s_branch2b'%name, 'bn%s_branch2b'%name],\n",
    "                       kernel_size=3, filters=filters, \n",
    "                       dilation_rate=dilation_rate, \n",
    "                       training=training)\n",
    "    out = conv_bn(out, ['res%s_branch2c'%name, 'bn%s_branch2c'%name],\n",
    "                  kernel_size=1, filters=filters_out, training=training)\n",
    "    if stride > 1 or filters_in != filters_out:\n",
    "        x = conv_bn(x, ['res%s_branch1'%name, 'bn%s_branch1'%name], \n",
    "                    kernel_size=1, filters=filters_out, stride=stride, training=training)\n",
    "    out = tf.nn.relu(x + out)\n",
    "    return out\n",
    "\n",
    "def res_block(x, num, filters, n_blocks, stride=1, dilation_rate=1, training=False):\n",
    "    names = ['{}{}'.format(num, ascii_lowercase[i]) for i in range(n_blocks)]\n",
    "    for i, name in enumerate(names):\n",
    "        x = conv_block(x, name, filters, stride=1 if i else stride, \n",
    "                       training=training, dilation_rate=dilation_rate)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplab_resnet50(x, training=False):\n",
    "    rate = 1 if training else 2\n",
    "    x = start_block(x)\n",
    "    conv2 = res_block(x, 2, filters=64, n_blocks=3, training=training)\n",
    "    x = res_block(conv2, 3, filters=128, n_blocks=4, stride=2, training=training)\n",
    "    x = res_block(x, 4, filters=256, n_blocks=6, stride=2 if training else 1, \n",
    "                  dilation_rate=rate, training=training)\n",
    "    x = res_block(x, 5, filters=512, n_blocks=3, dilation_rate=2*rate, training=training)\n",
    "    x = res_block(x, 6, filters=512, n_blocks=3, dilation_rate=4*rate, training=training)\n",
    "    x = res_block(x, 7, filters=512, n_blocks=3, dilation_rate=8*rate, training=training)\n",
    "    x = res_block(x, 8, filters=512, n_blocks=3, dilation_rate=16*rate, training=training)\n",
    "    return conv2, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspp(x, training=False):\n",
    "    rate = 1 if training else 2\n",
    "    feature_map1 = conv_bn(x, ['aspp_conv1', 'aspp_bn1'], 1, 256, training=training)\n",
    "    feature_map2 = conv_bn(x, ['aspp_conv2', 'aspp_bn2'], 3, 256, dilation_rate=6*rate, training=training)\n",
    "    feature_map3 = conv_bn(x, ['aspp_conv3', 'aspp_bn3'], 3, 256, dilation_rate=12*rate, training=training)\n",
    "    feature_map4 = conv_bn(x, ['aspp_conv4', 'aspp_bn4'], 3, 256, dilation_rate=18*rate, training=training)\n",
    "    global_features = tf.reduce_mean(x, axis=[1,2], keep_dims=True)\n",
    "    global_features = conv_bn(global_features, ['global_conv1', 'global_bn1'], 1, 256, training=training)\n",
    "    global_features = tf.image.resize_bilinear(images=global_features, size=tf.shape(feature_map1)[1:3])\n",
    "    concat = tf.concat([feature_map1, \n",
    "                        feature_map2, \n",
    "                        feature_map3, \n",
    "                        feature_map4, \n",
    "                        global_features], axis=-1)\n",
    "    out = conv_bn(concat, ['aspp_out_conv1', 'aspp_out_bn1'], 1, 256, training=training)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplab_encoder(x, training=False):\n",
    "    low_level, res_out = deeplab_resnet50(x, training)\n",
    "    aspp_out = aspp(res_out)\n",
    "    return low_level, aspp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplab_decoder(low_level, aspp_out, training=False):\n",
    "    low_level = conv_bn(low_level, ['dec_res_conv1', 'dec_res_bn1'], 1, 48, training=training)\n",
    "    aspp_out = tf.image.resize_bilinear(images=aspp_out, \n",
    "                                        size=tf.shape(aspp_out)[1:3]*(2+2*training))\n",
    "    x = tf.concat([low_level, aspp_out], axis=-1)\n",
    "    x = conv_bn(x, ['dec_conv1', 'dec_bn1'], 3, 256, training=training)\n",
    "    x = conv_bn(x, ['dec_conv2', 'dec_bn2'], 3, 256, training=training)\n",
    "    x = tf.image.resize_bilinear(images=x, size=tf.shape(x)[1:3]*4)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplab(x, n_classes, training=False):\n",
    "    low_level, aspp_out = deeplab_encoder(x, training)\n",
    "    dec_out = deeplab_decoder(low_level, aspp_out, training)\n",
    "    logits = conv_bn(dec_out, ['out_conv1', 'out_bn1'], kernel_size=1, filters=n_classes, training=training)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=w=256\n",
    "downsample = 2\n",
    "filespath = 'VOCdevkit/VOC2012/ImageSets/Segmentation'\n",
    "train = open(os.path.join(filespath, 'train.txt')).read().split('\\n')[:-1]\n",
    "val = open(os.path.join(filespath, 'val.txt')).read().split('\\n')[:-1]\n",
    "imgs_path = 'VOCdevkit/VOC2012/JPEGImages/{}.jpg'\n",
    "segs_path = 'VOCdevkit/VOC2012/SegmentationMap/{}.png'\n",
    "img_files, mask_files = [[path.format(f) for f in train] for path in [imgs_path, segs_path]]\n",
    "valid_img_files, valid_mask_files = [[path.format(f) for f in val] for path in [imgs_path, segs_path]]\n",
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "num_ex = len(img_files)\n",
    "num_valid = len(valid_img_files)\n",
    "batches_per_epoch = math.ceil(num_ex/batch_size)\n",
    "num_valid_batches = math.ceil(num_valid/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(img_file, img_type, height=None, width=None, downsample=None):\n",
    "    assert img_type in ['jpeg', 'png']\n",
    "    img_string = tf.read_file(img_file)\n",
    "    if img_type == 'jpeg':\n",
    "        img = tf.image.decode_jpeg(img_string)\n",
    "    if img_type == 'png':\n",
    "        img = tf.image.decode_png(img_string)\n",
    "    if downsample is not None:\n",
    "        img = img[::downsample, ::downsample]\n",
    "    if height is not None and width is not None:\n",
    "        img = tf.image.resize_image_with_crop_or_pad(img, height, width)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    return img\n",
    "    \n",
    "def read_imgs(files, dtypes, height=None, width=None, downsample=None):\n",
    "    imgs = [read_img(file, dtype, height, width, downsample) \n",
    "            for file, dtype in zip(files, dtypes)]\n",
    "    return imgs\n",
    "\n",
    "def make_dataset(img_files, mask_files, \n",
    "                 batch_size, n_epochs=None, \n",
    "                 height=None, width=None,\n",
    "                 downsample=None, shuffle=True):\n",
    "    dataset = tf.data.Dataset.zip(tuple(tf.data.Dataset.from_tensor_slices(files) \n",
    "                                        for files in [img_files, mask_files]))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(img_files))\n",
    "    \n",
    "    dataset = dataset.map(lambda x, y: read_imgs([x,y], ['jpeg', 'png'], height, width, downsample))\n",
    "    dataset = dataset.batch(batch_size).repeat(n_epochs)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_dataset = make_dataset(img_files, mask_files,batch_size,n_epochs,h,w,downsample)\n",
    "valid_dataset = make_dataset(valid_img_files, valid_mask_files,batch_size,n_epochs,h,w,downsample)\n",
    "\n",
    "dataset_handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(dataset_handle, \n",
    "                                              train_dataset.output_types,\n",
    "                                              train_dataset.output_shapes)\n",
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "valid_iterator = valid_dataset.make_one_shot_iterator()\n",
    "\n",
    "x, y = iterator.get_next()\n",
    "\n",
    "x.set_shape([batch_size, h, w, 3])\n",
    "y.set_shape([batch_size, h, w, 4])\n",
    "\n",
    "x = x/255\n",
    "y = y[...,0]\n",
    "\n",
    "\n",
    "is_train = tf.placeholder(name='is_train',dtype=tf.bool,shape=None)\n",
    "\n",
    "n_classes = 21\n",
    "weights = np.ones(n_classes)/n_classes\n",
    "with tf.variable_scope('model'):\n",
    "    logits = deeplab(x, n_classes=n_classes, training=False)\n",
    "preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "y_one_hot = tf.one_hot(tf.cast(y, tf.int32), depth=n_classes, axis=-1)\n",
    "preds_one_hot = tf.one_hot(preds, depth=n_classes, axis=-1)\n",
    "\n",
    "#mask = tf.expand_dims(1 - tf.cast(tf.equal(y,0), tf.float32), axis=-1)\n",
    "\n",
    "mask = tf.expand_dims(tf.ones_like(y), axis=-1)\n",
    "\n",
    "y_one_hot_masked = y_one_hot*mask\n",
    "preds_one_hot_masked = preds_one_hot*mask\n",
    "\n",
    "class_counts = tf.reduce_sum(y_one_hot, axis=[0,1,2])\n",
    "equal = tf.cast(tf.equal(tf.cast(y, tf.int32), preds), tf.float32)\n",
    "num_right = tf.reduce_sum(tf.expand_dims(equal,axis=-1)*y_one_hot_masked, axis=[0,1,2])\n",
    "class_acc = num_right/class_counts\n",
    "\n",
    "macc = tf.reduce_mean(equal)\n",
    "\n",
    "intersection = tf.reduce_sum(y_one_hot_masked*preds_one_hot_masked, axis=[1,2])\n",
    "union = tf.reduce_sum(y_one_hot_masked, axis=[1,2]) + tf.reduce_sum(preds_one_hot_masked, axis=[1,2]) - intersection\n",
    "iou = tf.reduce_mean((intersection + 1e-10)/(union + 1e-10), axis=0)\n",
    "miou = tf.reduce_mean(iou)\n",
    "\n",
    "\n",
    "logprobs = tf.nn.log_softmax(logits)\n",
    "losses = -tf.reduce_mean(logprobs*preds_one_hot_masked, axis=(0,1,2))\n",
    "loss = tf.reduce_sum(losses*weights)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    opt = tf.train.MomentumOptimizer(0.007,momentum=0.9)\n",
    "    train_step = opt.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from pho_seg_preproc_minus_convex_test_diff_new/notebooks/checkpoints/resnet50_ckpt\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_iou = []\n",
    "valid_iou = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "weights = np.load('resnet50.npz')\n",
    "assigned_vars = [v for v in tf.global_variables() if v.name.replace('model/','') in weights.keys()]\n",
    "unassigned_vars = set(tf.global_variables()) - set(assigned_vars)\n",
    "\n",
    "deeplab_saver = tf.train.Saver(var_list = assigned_vars)\n",
    "deeplab_saver.restore(sess, 'pho_seg_preproc_minus_convex_test_diff_new/notebooks/checkpoints/resnet50_ckpt')\n",
    "sess.run(tf.variables_initializer(unassigned_vars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "print('Training')\n",
    "\n",
    "for i in range(1,n_epochs+1):\n",
    "    training_handle = sess.run(train_iterator.string_handle())\n",
    "    num_so_far = 0\n",
    "    for n in range(1,batches_per_epoch+1):\n",
    "        _, t_loss, t_iou, t_acc = sess.run([train_step, loss, miou, macc], \n",
    "            {dataset_handle:training_handle, is_train:True})\n",
    "        \n",
    "        train_loss.append(t_loss)\n",
    "        train_iou.append(t_iou)\n",
    "        train_acc.append(t_acc)\n",
    "        num_so_far += batch_size\n",
    "        mean_loss = np.mean(train_loss[-n:])\n",
    "        mean_iou = np.mean(train_iou[-n:])\n",
    "        mean_acc = np.mean(train_acc[-n:])\n",
    "        sys.stdout.write('\\rTraining: epoch {}/{}, batch {}/{}, loss:{:.4f}, iou:{:.4f}, acc:{:.4f}'.format(\n",
    "            i,n_epochs,n,batches_per_epoch, mean_loss, mean_iou, mean_acc))\n",
    "    \n",
    "    validation_handle = sess.run(valid_iterator.string_handle())\n",
    "    \n",
    "    sys.stdout.write('\\r\\n')\n",
    "    \n",
    "    \n",
    "    v_loss = []\n",
    "    v_iou = []\n",
    "    v_acc = []\n",
    "    for n in range(1,num_valid_batches+1):\n",
    "        v_l, v_i, v_a = sess.run([loss, miou, macc], {dataset_handle:validation_handle, is_train:False})\n",
    "        v_loss.append(v_l)\n",
    "        v_iou.append(v_i)\n",
    "        v_acc.append(v_a)\n",
    "        \n",
    "        out_str = 'Validation: batch {}/{}'.format(n,num_valid_batches)\n",
    "        sys.stdout.write('\\r{}'.format(out_str))\n",
    "        \n",
    "    mean_loss = np.mean(v_loss)\n",
    "    mean_iou = np.mean(v_iou)\n",
    "    mean_acc = np.mean(v_acc)\n",
    "    \n",
    "    valid_loss.append(mean_loss)\n",
    "    valid_iou.append(mean_iou)\n",
    "    valid_acc.append(mean_acc)\n",
    "    sys.stdout.write('\\r{}, loss: {:.4f}, iou: {:.4f}, acc:{:.4f}'.format(out_str, mean_loss, mean_iou, mean_acc))\n",
    "    sys.stdout.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
